{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Drive Configuration"
      ],
      "metadata": {
        "id": "BtTPJTtv53VD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2uatWCx4O32"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/ForestSounds\n",
        "!pip install librosa torchaudio matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion to spectograms"
      ],
      "metadata": {
        "id": "cevQMSeN5ao7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "SAMPLE_RATE = 22050\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 256\n",
        "DURATION = 4  # secs/clip\n",
        "\n",
        "# Paths\n",
        "driveRawAudioPath = '/content/drive/MyDrive/ForestSounds/raw_audio'\n",
        "outputFolderPath = '/content/drive/MyDrive/ForestSounds/training_spectrograms'\n",
        "os.makedirs(outputFolderPath, exist_ok=True)\n",
        "\n",
        "audioFiles = []\n",
        "\n",
        "print(\"Check raw_audio directory...\")\n",
        "path = Path(driveRawAudioPath)\n",
        "\n",
        "if path.exists():\n",
        "    print(f\"Directory exists: {driveRawAudioPath}\")\n",
        "    audioFiles = list(path.rglob(\"*.wav\")) + list(path.rglob(\"*.mp3\")) + list(path.rglob(\"*.flac\")) + list(path.rglob(\"*.ogg\"))\n",
        "    print(f\"Found {len(audioFiles)} audio file(s).\")\n",
        "    if len(audioFiles) == 0:\n",
        "        print(\"No audio files found.\")\n",
        "    else:\n",
        "        print(f\"Files found: {[f.name for f in audioFiles]}\")\n",
        "else:\n",
        "    print(f\"Directory not found at {driveRawAudioPath}\")\n",
        "\n",
        "if len(audioFiles) > 0:\n",
        "    print(\"\\n Starting spectrogram creation...\")\n",
        "\n",
        "    clipCount = 0\n",
        "    for audioFile in audioFiles:\n",
        "        print(f\"Processing: {audioFile.name}...\")\n",
        "        try:\n",
        "            y, sr = librosa.load(audioFile, sr=SAMPLE_RATE)\n",
        "            samplesPerClip = SAMPLE_RATE * DURATION\n",
        "            numClips = len(y) // samplesPerClip\n",
        "            print(f\"  -> Split into {numClips} clips.\")\n",
        "\n",
        "            for i in range(numClips):\n",
        "                startSample = i * samplesPerClip\n",
        "                endSample = startSample + samplesPerClip\n",
        "                clip = y[startSample:endSample]\n",
        "\n",
        "                # Generate Mel-spectrogram\n",
        "                melSpec = librosa.feature.melspectrogram(y=clip, sr=sr, n_fft=N_FFT,\n",
        "                                                          hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
        "                logMelSpec = librosa.power_to_db(melSpec, ref=np.max)\n",
        "                logMelSpec = np.clip(logMelSpec, -80, 0) / 80 * 2 + 1\n",
        "                outputFilename = f\"spec_{clipCount:05d}.npy\"\n",
        "                outputPath = os.path.join(outputFolderPath, outputFilename)\n",
        "                np.save(outputPath, logMelSpec)\n",
        "                clipCount += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audioFile.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n Created {clipCount} total spectrogram clips.\")\n",
        "    print(f\"Saved in: {outputFolderPath}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n Stopped. No valid audio files were found to process.\")"
      ],
      "metadata": {
        "id": "N3Yd0zcMZRka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build GAN model"
      ],
      "metadata": {
        "id": "bPHuoeGt5i5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Random noise into a spectrogram\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latentDim=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latentDim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Discriminator (real or fake)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)\n",
        "\n",
        "latentDim = 100\n",
        "netG = Generator(latentDim).to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "print(\"Generator input shape (latent noise):\", (1, latentDim, 1, 1))\n",
        "\n",
        "# Test Generator with fake input\n",
        "testNoise = torch.randn(1, latentDim, 1, 1, device=device)\n",
        "testOutput = netG(testNoise)\n",
        "print(\"Generator output shape:\", testOutput.shape)\n",
        "\n",
        "# Test Discriminator with the generator's output\n",
        "dOutput = netD(testOutput)\n",
        "print(\"Discriminator output shape (should be 1):\", dOutput.shape)\n",
        "print(\"Discriminator output value (should be between 0-1):\", dOutput.item())\n",
        "\n",
        "print(\"Model build and shape test completed!\\n\")\n",
        "\n",
        "# Model summaries\n",
        "print(\"Generator Architecture:\")\n",
        "print(netG)\n",
        "print(\"\\nDiscriminator Architecture:\")\n",
        "print(netD)"
      ],
      "metadata": {
        "id": "K1ARmVor5lTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "4qvothcg5mua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, folderPath, targetSize=(64, 64)):\n",
        "        self.folderPath = folderPath\n",
        "        self.filePaths = [os.path.join(folderPath, f) for f in os.listdir(folderPath) if f.endswith('.npy')]\n",
        "        self.targetSize = targetSize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filePaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec = np.load(self.filePaths[idx])\n",
        "        specTensor = torch.FloatTensor(spec).unsqueeze(0)\n",
        "        specTensorResized = F.interpolate(specTensor.unsqueeze(0), size=self.targetSize, mode='bilinear', align_corners=False).squeeze(0)\n",
        "        return specTensorResized\n",
        "\n",
        "datasetPath = '/content/drive/MyDrive/ForestSounds/training_spectrograms'\n",
        "dataset = SpectrogramDataset(datasetPath, targetSize=(64, 64))\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "firstBatch = next(iter(dataloader))\n",
        "print(f\"Batch shape: {firstBatch.shape}\")\n",
        "\n",
        "print(f\"Loaded {len(dataset)} spectrogram samples for training.\")\n",
        "\n",
        "# Clean slate\n",
        "latentDim = 100\n",
        "netG = Generator(latentDim).to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "gLosses = []\n",
        "dLosses = []\n",
        "\n",
        "fixedNoise = torch.randn(16, latentDim, 1, 1, device=device)\n",
        "\n",
        "os.makedirs('training_samples', exist_ok=True)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "numEpochs = 500\n",
        "\n",
        "for epoch in range(numEpochs):\n",
        "    for i, realData in enumerate(dataloader):\n",
        "        netD.zero_grad()\n",
        "\n",
        "        realData = realData.to(device)\n",
        "        batchSize = realData.size(0)\n",
        "        realLabels = torch.ones(batchSize, device=device)\n",
        "\n",
        "        outputReal = netD(realData)\n",
        "        lossDReal = criterion(outputReal, realLabels)\n",
        "\n",
        "        # Fake data\n",
        "        noise = torch.randn(batchSize, latentDim, 1, 1, device=device)\n",
        "        fakeData = netG(noise)\n",
        "        fakeLabels = torch.zeros(batchSize, device=device)\n",
        "\n",
        "        outputFake = netD(fakeData.detach())\n",
        "        lossDFake = criterion(outputFake, fakeLabels)\n",
        "\n",
        "        lossD = lossDReal + lossDFake\n",
        "        lossD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        output = netD(fakeData)\n",
        "        lossG = criterion(output, realLabels)\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        gLosses.append(lossG.item())\n",
        "        dLosses.append(lossD.item())\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{numEpochs}]\\tLoss_D: {lossD.item():.4f}\\tLoss_G: {lossG.item():.4f}')\n",
        "\n",
        "    # Every 10 epochs, save sample of generated spectrograms\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        with torch.no_grad():\n",
        "            fakeSamples = netG(fixedNoise).detach().cpu()\n",
        "\n",
        "        # Plot samples\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            if i < 8:\n",
        "                spec = fakeSamples[i].squeeze().numpy()\n",
        "                ax.imshow(spec, aspect='auto', origin='lower', cmap='viridis')\n",
        "                ax.axis('off')\n",
        "        plt.suptitle(f'Generated Spectrograms - Epoch {epoch+1}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'training_samples/epoch_{epoch+1:03d}.png')\n",
        "        plt.show()\n",
        "\n",
        "        # Save the model checkpoint\n",
        "        torch.save(netG.state_dict(), f'generator_epoch_{epoch+1}.pth')\n",
        "        print(f'Saved sample images and model checkpoint at epoch {epoch+1}')\n",
        "\n",
        "# Plot loss history after training\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(gLosses, label=\"G\")\n",
        "plt.plot(dLosses, label=\"D\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig('training_loss.png')\n",
        "plt.show()\n",
        "\n",
        "print(\"Training finished!\")"
      ],
      "metadata": {
        "id": "Pdk2BOzE5qBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Sound Generation"
      ],
      "metadata": {
        "id": "ftmi4Qzx5vZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netG.eval()\n",
        "\n",
        "def generateCustomSound(model, totalDuration=30, clipDuration=4, outputPrefix=\"ai_forest\"):\n",
        "    \"\"\"\n",
        "    Generates a sound of any duration by stitching multiple clips together.\n",
        "\n",
        "    Args:\n",
        "        model: Your trained Generator model.\n",
        "        totalDuration: Total length of the final sound in seconds.\n",
        "        clipDuration: Length of each individual generated clip in seconds.\n",
        "        outputPrefix: Base name for the output files.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    from scipy.io import wavfile\n",
        "\n",
        "    numClips = math.ceil(totalDuration / clipDuration)\n",
        "    actualDuration = numClips * clipDuration\n",
        "    print(f\" {numClips} clips ({clipDuration}s each) for a {actualDuration}s total sound\")\n",
        "\n",
        "    allAudio = np.array([])\n",
        "    individualClips = []\n",
        "\n",
        "    for i in range(numClips):\n",
        "        with torch.no_grad():\n",
        "            # Generate a new random spectrogram for each clip\n",
        "            noise = torch.randn(1, 100, 1, 1, device=device)\n",
        "            fakeSpec = netG(noise).cpu().numpy().squeeze()\n",
        "\n",
        "        # Convert the spectrogram back to audio\n",
        "        spec = (fakeSpec - 1) / 2 * 80 - 80\n",
        "        specPower = librosa.db_to_power(spec)\n",
        "        audio = librosa.feature.inverse.mel_to_audio(specPower, sr=SAMPLE_RATE,\n",
        "                                                    n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
        "\n",
        "        # Trim longer clips\n",
        "        if len(audio) > SAMPLE_RATE * clipDuration:\n",
        "            audio = audio[:SAMPLE_RATE * clipDuration]\n",
        "\n",
        "        # Save individual clip\n",
        "        clipFilename = f\"{outputPrefix}_clip_{i+1:03d}.wav\"\n",
        "        audioNormalized = np.int16(audio / np.max(np.abs(audio)) * 32767)\n",
        "        wavfile.write(clipFilename, SAMPLE_RATE, audioNormalized)\n",
        "        individualClips.append(clipFilename)\n",
        "\n",
        "        # Append clip to main audio\n",
        "        allAudio = np.concatenate((allAudio, audio))\n",
        "        print(f\"  Generated clip {i+1}/{numClips} -> {clipFilename}\")\n",
        "\n",
        "    # Save long, combined audio file\n",
        "    longFilename = f\"{outputPrefix}_full_{actualDuration}s.wav\"\n",
        "    audioNormalized = np.int16(allAudio / np.max(np.abs(allAudio)) * 32767)\n",
        "    wavfile.write(longFilename, SAMPLE_RATE, audioNormalized)\n",
        "\n",
        "    print(f\"\\n Generation complete!\")\n",
        "    print(f\"   Full sound: {longFilename} ({actualDuration}s)\")\n",
        "    print(f\"   Individual clips: {numClips} files\")\n",
        "\n",
        "    return longFilename, individualClips\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"OGenerating long ambient soundscape\")\n",
        "print(\"=\"*50)\n",
        "full_sound, clips = generateCustomSound(netG, totalDuration=1200, clipDuration=15, outputPrefix=\"ambient\")\n",
        "\n",
        "print(\"\\n All sounds generated!\")"
      ],
      "metadata": {
        "id": "-VrMCBhK5x1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-processing"
      ],
      "metadata": {
        "id": "rcWMcmqbh4Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "import soundfile as sf\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def postprocessAudioFile(inputFilename, outputFilename=None,\n",
        "                          fadeDuration=3.0, noiseIntensity=0.01,\n",
        "                          cutoffFreq=5000):\n",
        "    if outputFilename is None:\n",
        "        base, ext = os.path.splitext(inputFilename)\n",
        "        outputFilename = f\"{base}_polished{ext}\"\n",
        "\n",
        "    print(f\" Processing: {os.path.basename(inputFilename)}\")\n",
        "\n",
        "    sampleRate, audioData = wavfile.read(inputFilename)\n",
        "    audioFloat = audioData.astype(np.float32) / 32768.0\n",
        "\n",
        "    # Fade in/out\n",
        "    fadeLength = int(sampleRate * fadeDuration)\n",
        "    if len(audioFloat) > fadeLength * 2:\n",
        "        fadeIn = np.linspace(0, 1, fadeLength)\n",
        "        fadeOut = np.linspace(1, 0, fadeLength)\n",
        "        audioFloat[:fadeLength] = audioFloat[:fadeLength] * fadeIn\n",
        "        audioFloat[-fadeLength:] = audioFloat[-fadeLength:] * fadeOut\n",
        "\n",
        "    # Background ambience\n",
        "    brownNoise = np.cumsum(np.random.randn(len(audioFloat)))\n",
        "    brownNoise = brownNoise / np.max(np.abs(brownNoise))\n",
        "    audioFloat = audioFloat + (brownNoise * noiseIntensity)\n",
        "\n",
        "    # Gentle EQ (low-pass filter)\n",
        "    sos = signal.butter(4, cutoffFreq, 'lowpass', fs=sampleRate, output='sos')\n",
        "    audioFloat = signal.sosfilt(sos, audioFloat)\n",
        "\n",
        "    # Normalize and save\n",
        "    audioFloat = audioFloat / np.max(np.abs(audioFloat))\n",
        "    sf.write(outputFilename, audioFloat, sampleRate)\n",
        "\n",
        "    return audioFloat, sampleRate, outputFilename\n",
        "\n",
        "def postprocessDirectory(filePattern, outputSuffix=\"_polished\", **kwargs):\n",
        "    audioFiles = glob.glob(filePattern)\n",
        "    print(f\"Found {len(audioFiles)} files to process matching '{filePattern}'\")\n",
        "\n",
        "    results = []\n",
        "    for audioFile in audioFiles:\n",
        "        base, ext = os.path.splitext(audioFile)\n",
        "        outputFile = f\"{base}{outputSuffix}{ext}\"\n",
        "\n",
        "        processedAudio, sr, outputPath = postprocessAudioFile(\n",
        "            audioFile, outputFile, **kwargs\n",
        "        )\n",
        "        results.append(outputPath)\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"Audio Post-Processing\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "longResult = []\n",
        "clipResults = []\n",
        "allResults = []\n",
        "\n",
        "print(\"-\" * 30)\n",
        "longResult = postprocessAudioFile(\n",
        "    \"ambient_full_1200s.wav\",\n",
        "    \"ambient_polished.wav\",\n",
        "    fadeDuration=5.0,\n",
        "    noiseIntensity=0.008,\n",
        "    cutoffFreq=4000\n",
        ")\n",
        "print(f\"Created: {longResult[2]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Post-processing completed\")\n",
        "print(\"Generated files:\")\n",
        "\n",
        "allProcessedFiles = []\n",
        "if longResult and len(longResult) > 2:\n",
        "    allProcessedFiles.append(longResult[2])\n",
        "if clipResults:\n",
        "    allProcessedFiles.extend(clipResults)\n",
        "if allResults:\n",
        "    allProcessedFiles.extend(allResults)\n",
        "\n",
        "# Print the results\n",
        "for file in allProcessedFiles:\n",
        "    print(f\"  - {file}\")\n",
        "\n",
        "# If no files were processed, show a message\n",
        "if not allProcessedFiles:\n",
        "    print(\"  No files were processed.\")\n",
        "\n",
        "# Bonus: Create a preview if we have any files\n",
        "if allProcessedFiles:\n",
        "    print(f\"\\n Playing preview of 1st processed file: {allProcessedFiles[0]}\")\n",
        "    from IPython.display import Audio\n",
        "    display(Audio(allProcessedFiles[0]))\n",
        "else:\n",
        "    print(\"\\n No files available for preview\")"
      ],
      "metadata": {
        "id": "15v8iliuh7is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}